# Pipeline Architecture Overview

**Complete architecture map of blog-writer**

---

## ğŸ—ï¸ High-Level Architecture

```
blog-writer/
â”‚
â”œâ”€â”€ ğŸ“¦ KEYWORD GENERATION (pipeline/keyword_generation/)
â”‚   â””â”€â”€ Standalone keyword research system
â”‚
â”œâ”€â”€ ğŸ“ BLOG GENERATION (pipeline/blog_generation/)
â”‚   â””â”€â”€ 12-stage pipeline for article creation
â”‚
â”œâ”€â”€ ğŸ”§ CORE INFRASTRUCTURE (pipeline/core/)
â”‚   â””â”€â”€ Workflow orchestration engine
â”‚
â””â”€â”€ ğŸ› ï¸ SUPPORTING MODULES
    â”œâ”€â”€ pipeline/models/          # Data models & API clients
    â”œâ”€â”€ pipeline/processors/      # Data processors (sitemap, citations, etc.)
    â”œâ”€â”€ pipeline/prompts/         # Prompt templates
    â””â”€â”€ pipeline/integrations/    # External API integrations
```

---

## 1ï¸âƒ£ KEYWORD GENERATION (`pipeline/keyword_generation/`)

**Location**: `/pipeline/keyword_generation/`

**Purpose**: Generate SEO keywords for companies

**Architecture**:
```
pipeline/keyword_generation/
â”œâ”€â”€ generator.py              # Main orchestrator (KeywordGeneratorV2)
â”œâ”€â”€ ai_generator.py           # AI-based keyword generation
â”œâ”€â”€ scorer.py                 # Keyword scoring with AI
â”œâ”€â”€ adapter.py                # Adapter for different keyword sources
â”œâ”€â”€ models.py                 # Data models (Keyword, CompanyInfo, etc.)
â”œâ”€â”€ config.py                # Configuration
â””â”€â”€ exceptions.py             # Custom exceptions
```

**Flow**:
```
1. Company Info Input
   â†“
2. AI Generator (50% keywords)
   â”œâ”€â”€ Seed keywords
   â””â”€â”€ Long-tail expansion
   â†“
3. Gap Analyzer (50% keywords)
   â”œâ”€â”€ Competitor analysis
   â””â”€â”€ SERanking API
   â†“
4. Merge & Deduplicate
   â†“
5. AI Scoring (all keywords)
   â†“
6. Filter & Sort
   â†“
7. Return KeywordGenerationResult
```

**Key Classes**:
- `KeywordGeneratorV2` - Main orchestrator
- `AIKeywordGenerator` - Generates keywords via Gemini
- `KeywordScorer` - Scores keywords with AI
- `GapAnalyzerWrapper` - Wraps SERanking API

**Integration Points**:
- `pipeline/integrations/seranking/` - SERanking API client
- `pipeline/models/gemini_client.py` - Gemini API for AI generation

**Usage**:
```python
from v2.keyword_generation import KeywordGeneratorV2

generator = KeywordGeneratorV2(
    google_api_key="...",
    seranking_api_key="..."
)

result = await generator.generate(
    company_info=CompanyInfo(name="...", url="..."),
    config=KeywordGenerationConfig()
)
```

---

## 2ï¸âƒ£ BLOG GENERATION (`pipeline/blog_generation/`)

**Location**: `/pipeline/blog_generation/`

**Purpose**: Generate complete blog articles via 12-stage pipeline

**Architecture**:
```
pipeline/blog_generation/
â”œâ”€â”€ stage_00_data_fetch.py      # Sequential: Fetch company data, sitemap
â”œâ”€â”€ stage_01_prompt_build.py    # Sequential: Build prompt with variables
â”œâ”€â”€ stage_02_gemini_call.py     # Sequential: Generate content (Gemini + tools)
â”œâ”€â”€ stage_03_extraction.py      # Sequential: Extract structured data
â”œâ”€â”€ stage_04_citations.py       # PARALLEL: Validate citations
â”œâ”€â”€ stage_05_internal_links.py  # PARALLEL: Generate internal links
â”œâ”€â”€ stage_06_toc.py             # PARALLEL: Table of contents
â”œâ”€â”€ stage_07_metadata.py        # PARALLEL: Calculate metadata
â”œâ”€â”€ stage_08_faq_paa.py         # PARALLEL: FAQ/PAA validation
â”œâ”€â”€ stage_09_image.py           # PARALLEL: Generate image
â”œâ”€â”€ stage_10_cleanup.py         # Sequential: Merge & validate
â””â”€â”€ stage_11_storage.py         # Sequential: HTML & storage
```

**Execution Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SEQUENTIAL PHASE 1: Foundation (Stages 0-3)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 0: Data Fetch
  â”œâ”€â”€ Fetch company info from URL
  â”œâ”€â”€ Crawl sitemap (SitemapCrawler)
  â””â”€â”€ Load job config
  â†“
Stage 1: Prompt Build
  â”œâ”€â”€ Load prompt template
  â”œâ”€â”€ Inject variables (keyword, company info, etc.)
  â””â”€â”€ Create final prompt
  â†“
Stage 2: Gemini Call
  â”œâ”€â”€ Call Gemini 3 Pro with tools
  â”œâ”€â”€ Tools: googleSearch + urlContext
  â””â”€â”€ Generate raw article (text/plain with JSON)
  â†“
Stage 3: Extraction
  â”œâ”€â”€ Parse JSON from text
  â”œâ”€â”€ Extract structured data
  â””â”€â”€ Validate structure

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARALLEL PHASE 2: Enhancements (Stages 4-9)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 4: Citations          â”‚  Stage 5: Internal Links
  â”œâ”€â”€ Validate URLs          â”‚    â”œâ”€â”€ Generate "More Reading"
  â”œâ”€â”€ Check accessibility     â”‚    â””â”€â”€ Link to sitemap pages
  â””â”€â”€ Format citations       â”‚
                             â”‚
Stage 6: ToC                 â”‚  Stage 7: Metadata
  â”œâ”€â”€ Extract headers         â”‚    â”œâ”€â”€ Calculate read time
  â””â”€â”€ Generate ToC labels     â”‚    â””â”€â”€ Set publish date
                             â”‚
Stage 8: FAQ/PAA             â”‚  Stage 9: Image
  â”œâ”€â”€ Validate Q&A           â”‚    â”œâ”€â”€ Generate prompt
  â””â”€â”€ Enhance if needed       â”‚    â””â”€â”€ Generate image (Replicate)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SEQUENTIAL PHASE 3: Finalization (Stages 10-11)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 10: Cleanup
  â”œâ”€â”€ Merge parallel results
  â”œâ”€â”€ Validate completeness
  â””â”€â”€ Final quality checks
  â†“
Stage 11: Storage
  â”œâ”€â”€ Generate HTML
  â”œâ”€â”€ Store in Supabase
  â””â”€â”€ Return final article
```

**Key Classes**:
- Each stage inherits from `Stage` base class
- All stages receive/return `ExecutionContext`
- Stages 4-9 run in parallel via `asyncio.gather()`

**Integration Points**:
- `pipeline/core/workflow_engine.py` - Orchestrates execution
- `pipeline/models/gemini_client.py` - Gemini API calls
- `pipeline/processors/sitemap_crawler.py` - Sitemap crawling (Stage 0)
- `pipeline/processors/citation_sanitizer.py` - Citation validation (Stage 4)
- `pipeline/processors/html_renderer.py` - HTML generation (Stage 11)
- `pipeline/prompts/main_article.py` - Prompt templates (Stage 1)

**Usage**:
```python
from v2.core import WorkflowEngine

engine = WorkflowEngine()
context = await engine.execute(
    job_id="blog-123",
    job_config={
        "primary_keyword": "AI adoption",
        "company_url": "https://example.com",
    }
)
```

---

## 3ï¸âƒ£ CORE INFRASTRUCTURE (`pipeline/core/`)

**Location**: `/pipeline/core/`

**Purpose**: Workflow orchestration and execution context

**Architecture**:
```
pipeline/core/
â”œâ”€â”€ workflow_engine.py        # Main orchestrator
â””â”€â”€ execution_context.py       # Shared data model
```

**WorkflowEngine**:
- Registers all 12 stages
- Executes stages in correct order (sequential â†’ parallel â†’ sequential)
- Handles errors and retries
- Manages execution context

**ExecutionContext**:
- Shared data model passed between stages
- Contains all intermediate results
- Fields: `prompt`, `raw_article`, `structured_data`, `citations`, etc.

---

## 4ï¸âƒ£ SUPPORTING MODULES

### Models (`pipeline/models/`)
- `gemini_client.py` - Gemini API wrapper
- `sitemap_page.py` - SitemapPage, SitemapPageList models
- `citation.py` - Citation models
- `toc.py` - Table of contents models
- `metadata.py` - Metadata models
- `faq_paa.py` - FAQ/PAA models
- `image_generator.py` - Image generation client

### Processors (`pipeline/processors/`)
- `sitemap_crawler.py` - **Sitemap crawling** (used in Stage 0)
- `citation_sanitizer.py` - Citation validation (Stage 4)
- `cleanup.py` - Content cleanup (Stage 10)
- `html_renderer.py` - HTML generation (Stage 11)
- `quality_checker.py` - Quality validation
- `storage.py` - Supabase storage

### Prompts (`pipeline/prompts/`)
- `main_article.py` - Main article generation prompt (Stage 1)
- `image_prompt.py` - Image generation prompt (Stage 9)

### Integrations (`pipeline/integrations/`)
- `seranking/` - SERanking API client (for keyword generation)

---

## ğŸ”„ Data Flow

### Keyword Generation Flow
```
Input: CompanyInfo
  â†“
KeywordGeneratorV2.generate()
  â”œâ”€â”€ AI Generator â†’ 40 keywords
  â”œâ”€â”€ Gap Analyzer â†’ 40 keywords
  â”œâ”€â”€ Merge & Deduplicate
  â”œâ”€â”€ Score all keywords
  â””â”€â”€ Filter & Sort
  â†“
Output: KeywordGenerationResult
```

### Blog Generation Flow
```
Input: JobConfig (keyword, company_url, etc.)
  â†“
WorkflowEngine.execute()
  â”œâ”€â”€ Stage 0: Fetch data (sitemap, company info)
  â”œâ”€â”€ Stage 1: Build prompt
  â”œâ”€â”€ Stage 2: Generate content (Gemini + tools)
  â”œâ”€â”€ Stage 3: Extract structured data
  â”œâ”€â”€ Stages 4-9: Parallel enhancements
  â”œâ”€â”€ Stage 10: Cleanup & validate
  â””â”€â”€ Stage 11: Generate HTML & store
  â†“
Output: ExecutionContext (with final_article)
```

---

## ğŸ“Š Key Differences: Pipeline vs V1

| Feature | V1 (src/) | Pipeline (pipeline/) |
|---------|-----------|----------|
| **Architecture** | Monolithic generators | 12-stage pipeline |
| **Keyword Gen** | Mixed with blog gen | Separate module |
| **Blog Gen** | Single ContentGenerator | 12 stages |
| **Sitemap** | Basic checker | Full crawler with classification |
| **Testing** | Limited | Comprehensive stage tests |
| **Modularity** | Low | High (each stage independent) |

---

## ğŸ¯ Entry Points

### Keyword Generation
```python
from v2.keyword_generation import KeywordGeneratorV2

generator = KeywordGeneratorV2(...)
result = await generator.generate(company_info, config)
```

### Blog Generation
```python
from v2.core import WorkflowEngine

engine = WorkflowEngine()
context = await engine.execute(job_id, job_config)
```

### Combined (Future)
```python
# 1. Generate keywords
keywords = await keyword_generator.generate(...)

# 2. Generate blogs for each keyword
for keyword in keywords:
    blog = await workflow_engine.execute(job_id, {
        "primary_keyword": keyword.keyword,
        ...
    })
```

---

## ğŸ“ Complete File Structure

```
pipeline/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ workflow_engine.py          # Orchestrator
â”‚   â””â”€â”€ execution_context.py        # Shared data model
â”‚
â”œâ”€â”€ keyword_generation/              # KEYWORD GENERATION
â”‚   â”œâ”€â”€ generator.py                # Main orchestrator
â”‚   â”œâ”€â”€ ai_generator.py             # AI keyword gen
â”‚   â”œâ”€â”€ scorer.py                   # Keyword scoring
â”‚   â”œâ”€â”€ adapter.py                  # Source adapter
â”‚   â”œâ”€â”€ models.py                   # Data models
â”‚   â””â”€â”€ config.py                   # Config
â”‚
â”œâ”€â”€ stages/                          # BLOG GENERATION (12 stages)
â”‚   â”œâ”€â”€ stage_00_data_fetch.py      # Data fetch
â”‚   â”œâ”€â”€ stage_01_prompt_build.py     # Prompt build
â”‚   â”œâ”€â”€ stage_02_gemini_call.py      # Content generation
â”‚   â”œâ”€â”€ stage_03_extraction.py      # Data extraction
â”‚   â”œâ”€â”€ stage_04_citations.py       # Citations (parallel)
â”‚   â”œâ”€â”€ stage_05_internal_links.py   # Internal links (parallel)
â”‚   â”œâ”€â”€ stage_06_toc.py              # ToC (parallel)
â”‚   â”œâ”€â”€ stage_07_metadata.py         # Metadata (parallel)
â”‚   â”œâ”€â”€ stage_08_faq_paa.py         # FAQ/PAA (parallel)
â”‚   â”œâ”€â”€ stage_09_image.py           # Image (parallel)
â”‚   â”œâ”€â”€ stage_10_cleanup.py         # Cleanup
â”‚   â””â”€â”€ stage_11_storage.py         # Storage
â”‚
â”œâ”€â”€ models/                         # Data models & clients
â”‚   â”œâ”€â”€ gemini_client.py            # Gemini API
â”‚   â”œâ”€â”€ sitemap_page.py            # Sitemap models
â”‚   â”œâ”€â”€ citation.py                # Citation models
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ processors/                     # Data processors
â”‚   â”œâ”€â”€ sitemap_crawler.py         # Sitemap crawler â­
â”‚   â”œâ”€â”€ citation_sanitizer.py     # Citation validation
â”‚   â”œâ”€â”€ html_renderer.py           # HTML generation
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ prompts/                        # Prompt templates
â”‚   â”œâ”€â”€ main_article.py            # Main prompt
â”‚   â””â”€â”€ image_prompt.py            # Image prompt
â”‚
â””â”€â”€ integrations/                   # External APIs
    â””â”€â”€ seranking/                 # SERanking API
```

---

## âœ… Summary

**Keyword Generation**: `pipeline/keyword_generation/` - Standalone module  
**Blog Generation**: `pipeline/blog_generation/` - 12-stage pipeline  
**Orchestration**: `pipeline/core/workflow_engine.py` - Executes stages  
**Sitemap Crawler**: `pipeline/processors/sitemap_crawler.py` - Used in Stage 0

Both systems are **independent** but can be used together for complete blog generation workflows.

